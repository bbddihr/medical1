{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "# 데이터를 크롤링하는 함수 정의\n",
    "def data_mining(url_list):\n",
    "    title_list = []  # 제목을 저장할 리스트\n",
    "    category_list = []  # 분류를 저장할 리스트\n",
    "    content_list = []  # 개요 내용을 저장할 리스트\n",
    "    # URL 리스트에서 하나씩 가져와서 크롤링\n",
    "    for url in url_list:\n",
    "        # 브라우저 열기\n",
    "        browser = webdriver.Chrome()\n",
    "        browser.get(url)\n",
    "        time.sleep(random.random() + 3)  # 랜덤한 시간 동안 대기\n",
    "        # 제목 크롤링\n",
    "        try:\n",
    "            title = browser.find_element(By.TAG_NAME, 'h1').text\n",
    "            title_list.append(title)\n",
    "        except:\n",
    "            title_list.append('없음')  # 제목이 없을 경우 '없음' 추가\n",
    "        time.sleep(random.random())\n",
    "        # 분류 크롤링\n",
    "        try:\n",
    "            category = browser.find_element(By.TAG_NAME, 'ul').text\n",
    "            category_list.append(category)\n",
    "        except:\n",
    "            category_list.append('없음')  # 분류가 없을 경우 '없음' 추가\n",
    "        time.sleep(random.random())\n",
    "        # 개요 내용 크롤링\n",
    "        try:\n",
    "            content = browser.find_element(By.CLASS_NAME, 'nA6im+12').text\n",
    "            content_list.append(content)\n",
    "        except:\n",
    "            content_list.append('없음')  # 내용이 없을 경우 '없음' 추가\n",
    "        time.sleep(random.random())\n",
    "        # 브라우저 닫기\n",
    "        browser.quit()\n",
    "    return title_list, category_list, content_list  # 크롤링한 데이터를 반환\n",
    "# 주어진 URL에서 최근 변경 사항을 가져오는 부분\n",
    "browser = webdriver.Chrome()\n",
    "url = 'https://namu.wiki/RecentChanges'\n",
    "browser.get(url)\n",
    "time.sleep(3)\n",
    "# 최근 변경 사항 목록에서 데이터 추출\n",
    "data_list = browser.find_elements(By.CLASS_NAME, 'b5G6-Ki+')[1:]\n",
    "url_list = []\n",
    "for data in data_list[:50]:  # 상위 50개의 URL만 사용\n",
    "    elem = data.find_element(By.TAG_NAME, 'a')\n",
    "    url_list.append(elem.get_attribute('href'))\n",
    "# 브라우저 닫기\n",
    "browser.quit()\n",
    "# URL 리스트를 사용하여 크롤링 수행\n",
    "title_l, cate_l, cont_l = data_mining(url_list)\n",
    "# 데이터를 데이터프레임으로 변환\n",
    "title_df = pd.DataFrame(title_l, columns=['Title'])\n",
    "category_df = pd.DataFrame(cate_l, columns=['Category'])\n",
    "content_df = pd.DataFrame(cont_l, columns=['Content'])\n",
    "# 데이터프레임을 하나로 결합\n",
    "df = pd.concat([title_df, category_df, content_df], axis=1)\n",
    "# 결과를 CSV 파일로 저장\n",
    "df.to_csv('namu.csv', index=False)  # 인덱스를 제외하고 저장"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
